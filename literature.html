<!DOCTYPE html>
<html>

<head>
    <title>Project Group 14</title>
    <meta charset="utf-8" />
  <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        displayAlign: 'center',
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
    </script>
    <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
    
    <style type="text/css">
        nav.sitenav ul {
            list-style: none;
        }
        
        nav.sitenav li {
            display: inline-block;
            width: 8em;
            text-align: center;
        }
        
        nav.sitenav a {
            display: block;
            font-weight: bold;
        }
        
        nav.sitenav a:link,
        nav.sitenav a:visited,
        nav.sitenav a:hover,
        nav.sitenav a:active {
            color: green;
            text-decoration: none;
            border-bottom: thin solid transparent;
        }
        
        nav.sitenav a:hover {
            border-bottom: thin solid green;
        }
        
        nav.sitenav li.youarehere a {
            background-color: #cdfecd;
            border-bottom: thin solid green;
        }
        
        section,
        aside,
        article,
        footer {
            clear: both;
        }
        
        footer {
            border-top: thin solid green;
        }
    </style>
    <link rel="stylesheet" type="text/css" href="css/unsemantic-grid-responsive-tablet.css"/>
</head>

<body>
 <div class="grid-container">     
    <div class="grid-100">
     <header>
        <h1>CS109 A - Fall 2018: Project Group 14</h1></header>
     </div>
     <div class="grid-100">
    <nav class="sitenav">
        <ul>
            <li><a href="/18_109A_Group14/index.html">Home</a></li>
			<li class="youarehere"><a href="#">Literature Review</a></li>
            <li><a href="/18_109A_Group14/data_cleaning.html">Data Cleaning</a></li>
            <li><a href="/18_109A_Group14/data_processing_and_EDA.html">EDA</a></li>            
        </ul>
    </nav>
     </div>
     <div class="grid-50">
    <article>
        <h2>Literature Review</h2>	
		<section>			
			<h4>AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias</h4>
			<p>
			<strong>Citation: </strong>
            <u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Corbett-Davies%2C+S">Sam Corbett-Davies</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pierson%2C+E">Emma Pierson</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feller%2C+A">Avi Feller</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goel%2C+S">Sharad Goel</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huq%2C+A">Aziz Huq</a></u>, June 10th 2017
			<u><a href="https://arxiv.org/abs/1701.08230">https://arxiv.org/abs/1701.08230</a></u>
			<br>            
			<strong>What: </strong>
			Algorithms are regularly used to decide whether defendants awaiting trial are too dangerous to be released. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness.
			<br>
			<strong>How:</strong>
			In this paper the authors reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities.
			<br>
			<strong>Conclusion: </strong>
			By analyzing data from Broward County, the authors find that optimizing for public safety yields stark racial disparities; conversely, satisfying past fairness definitions means releasing more high-risk defendants, adversely affecting public safety. And algorithms have the potential to improve the efficiency and equity of decisions, but their design and application raise complex questions for researchers and policymakers.
			</p>
			<h4>Decoupled classifiers for fair and efficient machine learning</h4>
			<p>			
			<strong>Citation:</strong>
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dwork%2C+C"> Cynthia Dwork </a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Immorlica%2C+N">Nicole Immorlica</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kalai%2C+A+T">Adam Tauman Kalai</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leiserson%2C+M">Max Leiserson</a></u>,July 21, 2017 
			<u>https://arxiv.org/pdf/1707.06613v1.pdf</u>
			<br>
			<strong>What: </strong>The paper considers how to use a sensitive attribute such as gender or race to maximize fairness and accuracy, assuming it is legal and ethical.
			<br>
			<strong>How: </strong>In this paper they explore decoupled classification systems, in which a separate classifier is trained on each group.
			<br>
			<strong>Conclusion:</strong> Experiments demonstrate that decoupling can reduce the loss on some datasets for some potentially sensitive features
			</p>
			<h4>Evidence and Actions on Mortgage Market Disparities: Research, Fair lending Enforcement and Consumer Protection</h4>
			<p>
			<strong>Citation:</strong>Marsha J. Courchane &amp; Stephen L. Ross, 2018. "
			<u><a href="https://ideas.repec.org/p/hka/wpaper/2018-052.html">Evidence and Actions on Mortgage Market Disparities: Research, Fair Lending Enforcement and Consumer Protection</a></u>,"
			<u><a href="https://ideas.repec.org/s/hka/wpaper.html">Working Papers</a></u>2018-052, Human Capital and Economic Opportunity Working Group.    
			<u>http://web2.uconn.edu/economics/working/2018-14.pdf</u>
			<br>
			<strong>What:</strong>The authors present overviews of the research on discrimination in mortgage underwriting and pricing, the experiences of minority borrowers prior to and during the financial crisis, as well as discuss the history of legal cases alleging disparate treatment of minority borrowers.
			<br>
			<strong>How:</strong>By using the existing research and legal discussions as a background, they discuss and examine mortgage regulations as well as recent developments in the FinTech industry including machine learning.
			<br>
			<strong>Conclusion: </strong> The empirical evidence is clear that unexplained racial and ethnic differences in both mortgage underwriting and prices are relatively small when looking within lender and controlling for product attributes. Yet, race and ethnicity appear to play a role in determining mortgage market outcomes in the U.S. economy. The authors believe many factors, including information, attachment to financial markets, pre- application assistance levels and even shopping behavior contributed to the long-standing failure of prime qualified minority borrowers to access the conventional, conforming loan market.			
			</p>
			<h4>Peer-to-peer lending and bias in crowd decision-making</h4>
			<p>
			<strong>Citation: </strong>Singh P, Uparna J, Karampourniotis P, Horvat E-A, Szymanski B, Korniss G, et al. (2018) Peer-to-peer lending and bias in crowd decision-making. PLoS ONE 13(3): e0193007.
			<u><a href="https://doi.org/10.1371/journal.pone.0193007">https://doi.org/10.1371/journal.pone.0193007</a></u>
			<br>
			<strong>What:</strong>The authors investigate the “flat-world” hypothesis, which is the idea that globalization eventually leads to economic equality by analyzing crowdfinancing data, to see if it creates opportunities for the world’s poor.
			<br>
			<strong>How:</strong> The authors analyzed 600 000 peer-to-peer loans made to individual lenders in more than 220 countries to borrowers in 80 countries. They used regression analysis to predict bias in country-pair transactions based on variables such as GDP, geographical distance and so forth, checked for deviations of the co-country network of loans and checked the potential susceptibility of the network to shocks that could change the system’s ability potential for flatness.
			<br>
			<strong>Conclusion: </strong>The authors found continued and increased bias in an inter-country, peer-to-peer crowdfinancing network. The biases are reinforced and made stronger by the rapid growth of the platform itself (“rich gets richer” effect). However, by removing a few high-volume lenders or high-transaction links could cause the network’s flatness to increase significantly. In this way, the bias is directly linked with the dominance of a few big players.
			</p>		
			</section>
			<section>
			<h3>Conclusions from the EDA and literature review</h3>
			<p>The EDA and literature review indicated that we need to take some additional steps to pre-process the data and construct useful models. </p>
			<p>Firstly, the classes are imbalanced, with the majority (around three-quarters) of loans being fully paid. Having imbalanced data may give a false impression of the performance of the model, and can negatively affect the performance of some learning algorithms such as logistic regression. <b>We will therefore balance the training data by resampling to achieve a 50:50 split between the target classes.</b></p>
			<p>Secondly, the cost of a false positive is high in this particular example - i.e. the cost of a defaut is much greater (where the investor can potentially lose all the invested money), compared to the potential upside of making a successful loan (where the investor will gain the interest rate on repayments). This means that accuracy is not the best metric and we want to focus on limiting the number of false positives as much as possible. <b>Therefore, we will use a selection of metrics other than accuracy. These include: precision, recall, F1_score, and AUC (area under the ROC curve).</b> These are explained further when developing the functions in the modelling approach. 
			</p>
			<p>Lastly we plan to explore various metrics of fairness to test whether the outcomes of the algorithms lead to different results for protected vs unprotected classes. These metrics include equal opportunity, predictive equality and statistical parity. Again, these are explained in more detail in the modelling approach. We have chosen to implement these metrics as direct calculations. </p>
			<p>However, there are limitations in such metrics. For instance, they should be used in allocation of risk assessment problems with well-defined protected attributes, in which one would like to have some sort of statistical or mathematical notion of sameness. We will need to ensure that we do not create biases where there are none, or just by assuming different groups are reflected differently in the dataset.</p>
		</section>
		<section>
		<h3>Baseline model</h3>
		<p>As mentioned above, the dataset will be rebalanced to contain balanced classes. For the most simple baseline model, if a model simply assumes that all loans are fully paid, it will achieve 50% precision. This is our initial baseline.</p>
		</section>

	
    </article>
     </div>
    </div>
</body>

</html>