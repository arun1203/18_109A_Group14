<!DOCTYPE html>
<html>

<head>
    <title>Literature Review</title>
    <meta charset="utf-8" />
  <!-- mathjax config similar to math.stackexchange -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        displayAlign: 'center',
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
    </script>
    <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

    <style type="text/css">
        nav.sitenav ul {
            list-style: none;
        }

        nav.sitenav li {
            display: inline-block;
            width: 8em;
            text-align: center;
        }

        nav.sitenav a {
            display: block;
            font-weight: bold;
        }

        nav.sitenav a:link,
        nav.sitenav a:visited,
        nav.sitenav a:hover,
        nav.sitenav a:active {
            color: green;
            text-decoration: none;
            border-bottom: thin solid transparent;
        }

        nav.sitenav a:hover {
            border-bottom: thin solid green;
        }

        nav.sitenav li.youarehere a {
            background-color: #cdfecd;
            border-bottom: thin solid green;
        }

        section,
        aside,
        article,
        footer {
            clear: both;
        }

        footer {
            border-top: thin solid green;
        }
    </style>
    <link rel="stylesheet" type="text/css" href="css/unsemantic-grid-responsive-tablet.css"/>
</head>

<body>
 <div class="grid-container">
    <div class="grid-100">
     <header>
        <h1>CS109 A - Fall 2018: Project Group 14</h1></header>
     </div>
     <div class="grid-100">
    <nav class="sitenav">
        <ul>
            <li><a href="index.html">Home</a></li>
			<li class="youarehere"><a href="#">Literature Review</a></li>
            <li><a href="cleaning.html">Data Cleaning</a></li>
            <li><a href="eda.html">EDA</a></li>
            <li><a href="Model tuning and selection v7.html">Models</a></li>
            <li><a href="conclusion.html">Conslusion</a></li>
        </ul>
    </nav>
     </div>
     <div class="grid-50">
    <article>
        <h2>Literature Review</h2>
		<section>
            <h4>AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias</h4>
            <p>
                <strong>Citation: </strong>
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Bellamy%2C+R+K+E">Rachel K. E. Bellamy</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Dey%2C+K">Kuntal Dey</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Hind%2C+M">Michael Hind</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goel%2C+S">Samuel C. Hoffman</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huq%2C+A">Stephanie Houde</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Kannan%2C+K">Kalapriya Kannan</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Lohia%2C+P">Pranay Lohia</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Martino%2C+J">Jacquelyn Martino</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Mehta%2C+S">Sameep Mehta</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Mojsilovic%2C+A">Aleksandra Mojsilovic</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Nagar%2C+S">Seema Nagar</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Richards%2C+J">John Richards</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Saha%2C+D">Diptikalyan Saha</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Sattigeri%2C+P">Prasanna Sattigeri</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M">Moninder Singh</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Varshney%2C+K+R">Kush R. Varshney</a></u>,
                <u><a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y">Yunfeng Zhang</a></u>, October 3rd 2018
                <u><a href="https://arxiv.org/abs/1810.01943">https://arxiv.org/abs/1810.01943</a></u>
                <br>
                <strong>What: </strong>
                This paper introduces a new open source Python toolkit for algorithmic fairness
                <br>
                <strong>How:</strong>
                The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting, and to provide a common framework for fairness researchers to share and evaluate algorithms. The library contains at present 10 bias mitigation algorithms, and support a number of fairness metrics that can help mitigate bias in dataset and models.
            </p>
			<h4>Algorithmic decision making and the cost of fairness</h4>
			<p>
			<strong>Citation: </strong>
            <u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Corbett-Davies%2C+S">Sam Corbett-Davies</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pierson%2C+E">Emma Pierson</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feller%2C+A">Avi Feller</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goel%2C+S">Sharad Goel</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huq%2C+A">Aziz Huq</a></u>, June 10th 2017
			<u><a href="https://arxiv.org/abs/1701.08230">https://arxiv.org/abs/1701.08230</a></u>
			<br>
			<strong>What: </strong>
			Algorithms are regularly used to decide whether defendants awaiting trial are too dangerous to be released. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness.
			<br>
			<strong>How:</strong>
			In this paper the authors reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities.
			<br>
			<strong>Conclusion: </strong>
			By analyzing data from Broward County, the authors find that optimizing for public safety yields stark racial disparities; conversely, satisfying past fairness definitions means releasing more high-risk defendants, adversely affecting public safety. And algorithms have the potential to improve the efficiency and equity of decisions, but their design and application raise complex questions for researchers and policymakers.
			</p>
			<h4>Decoupled classifiers for fair and efficient machine learning</h4>
			<p>
			<strong>Citation:</strong>
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dwork%2C+C"> Cynthia Dwork </a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Immorlica%2C+N">Nicole Immorlica</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kalai%2C+A+T">Adam Tauman Kalai</a></u>,
			<u><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leiserson%2C+M">Max Leiserson</a></u>,July 21, 2017
			<u>https://arxiv.org/pdf/1707.06613v1.pdf</u>
			<br>
			<strong>What: </strong>The paper considers how to use a sensitive attribute such as gender or race to maximize fairness and accuracy, assuming it is legal and ethical.
			<br>
			<strong>How: </strong>In this paper they explore decoupled classification systems, in which a separate classifier is trained on each group.
			<br>
			<strong>Conclusion:</strong> Experiments demonstrate that decoupling can reduce the loss on some datasets for some potentially sensitive features
			</p>
			<h4>Evidence and Actions on Mortgage Market Disparities: Research, Fair lending Enforcement and Consumer Protection</h4>
			<p>
			<strong>Citation:</strong>Marsha J. Courchane &amp; Stephen L. Ross, 2018. "
			<u><a href="https://ideas.repec.org/p/hka/wpaper/2018-052.html">Evidence and Actions on Mortgage Market Disparities: Research, Fair Lending Enforcement and Consumer Protection</a></u>,"
			<u><a href="https://ideas.repec.org/s/hka/wpaper.html">Working Papers</a></u>2018-052, Human Capital and Economic Opportunity Working Group.
			<u>http://web2.uconn.edu/economics/working/2018-14.pdf</u>
			<br>
			<strong>What:</strong>The authors present overviews of the research on discrimination in mortgage underwriting and pricing, the experiences of minority borrowers prior to and during the financial crisis, as well as discuss the history of legal cases alleging disparate treatment of minority borrowers.
			<br>
			<strong>How:</strong>By using the existing research and legal discussions as a background, they discuss and examine mortgage regulations as well as recent developments in the FinTech industry including machine learning.
			<br>
			<strong>Conclusion: </strong> The empirical evidence is clear that unexplained racial and ethnic differences in both mortgage underwriting and prices are relatively small when looking within lender and controlling for product attributes. Yet, race and ethnicity appear to play a role in determining mortgage market outcomes in the U.S. economy. The authors believe many factors, including information, attachment to financial markets, pre- application assistance levels and even shopping behavior contributed to the long-standing failure of prime qualified minority borrowers to access the conventional, conforming loan market.
			</p>
			<h4>Peer-to-peer lending and bias in crowd decision-making</h4>
			<p>
			<strong>Citation: </strong>Singh P, Uparna J, Karampourniotis P, Horvat E-A, Szymanski B, Korniss G, et al. (2018) Peer-to-peer lending and bias in crowd decision-making. PLoS ONE 13(3): e0193007.
			<u><a href="https://doi.org/10.1371/journal.pone.0193007">https://doi.org/10.1371/journal.pone.0193007</a></u>
			<br>
			<strong>What:</strong>The authors investigate the “flat-world” hypothesis, which is the idea that globalization eventually leads to economic equality by analyzing crowdfinancing data, to see if it creates opportunities for the world’s poor.
			<br>
			<strong>How:</strong> The authors analyzed 600 000 peer-to-peer loans made to individual lenders in more than 220 countries to borrowers in 80 countries. They used regression analysis to predict bias in country-pair transactions based on variables such as GDP, geographical distance and so forth, checked for deviations of the co-country network of loans and checked the potential susceptibility of the network to shocks that could change the system’s ability potential for flatness.
			<br>
			<strong>Conclusion: </strong>The authors found continued and increased bias in an inter-country, peer-to-peer crowdfinancing network. The biases are reinforced and made stronger by the rapid growth of the platform itself (“rich gets richer” effect). However, by removing a few high-volume lenders or high-transaction links could cause the network’s flatness to increase significantly. In this way, the bias is directly linked with the dominance of a few big players.
			</p>
			</section>

    </article>
     </div>
    </div>
</body>

</html>
